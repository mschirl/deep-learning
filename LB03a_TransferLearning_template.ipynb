{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LB03a.0 Transfer Learning\n",
    "\n",
    "To fully train a complex convolutional neural network (CNN) from scratch requires a large dataset of sufficient size, which may be difficult to acquire. Additionally, such a CNN can take several weeks of training on multiple GPUs which poses a high demand of computing resources.\n",
    "\n",
    "It is a common procedure to use a pre-trained CNN as a feature extractor for the task of interest. Therefore, one can reuse the CNN's architecture with its trained weights and discard only the classification part of the architecture (fully connected layers). The classification part of the architecture has to be defined according to the classification problem to solve.\n",
    "\n",
    "<img src=\"resources/LB03a_transfer_learning.png\"/>\n",
    "\n",
    "### Different scenarios when using transfer learning ([medium.com](https://medium.com/@galen.ballew/transferlearning-b65772083b47)):\n",
    "\n",
    "**1) New dataset is small and similar to the previous**: Since the new data set is small, you run the risk of overfitting if you retrained everything. Instead, slice off the last fully connected layer and replace with with a new fully connected layer with the appropriate output size. This makes sense because the similarity of the observations (i.e. pictures) means both the low-level (e.g. edges) and high-level features (e.g. shapes) will be similiar. Freeze the weights before the last layer and retrain!\n",
    "\n",
    "**2) New data set is large and similar to the previous**: Since there is more data, there is less risk of overfitting by retraining. Freeze the low-level feature weights and retrain the high-level features to get a better generalization. Don’t forget to replace the last fully connected layer! *Optional: If your data set is large enough to handle it, you can initialize all the layers with their previous weights/biases and retrain the entire network.*\n",
    "\n",
    "**3) New data set is small and different than the previous**: This is the most difficult situation to deal with. Intuitively, we know that the previous network is finely-tuned at each layer. However, we do not want any of the high-level features and we cannot afford to retrain them because we could overfit. Instead, remove all of the fully connected layers and all of the high-level convolutional layers. All that should remain are the first few low-level convolutional layers. Place a fully connected layer with the correct number of outputs, freeze the rest of the layers, and retrain. \n",
    "\n",
    "**4) New data set is large and different than the previous**: Retrain the entire network. It’s usually a good idea to instantiate the previous model’s weights/biases to speed up training (lot’s of the low-level convolutions will have similiar weights/biases). Don’t forget to replace the fully connected output layer.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Develop an image processing system that is capable of distinguishing between two types of leucocytes (white blood cells): lymphocytes and neutrophils.\n",
    "\n",
    "<img src=\"resources/LB03a_blood_cells.png\"/>\n",
    "\n",
    "Use the technique of transfer learning with bottlenecking and aim for a stable accuracy of >92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "\n",
    "    from keras import __version__\n",
    "    from keras import optimizers\n",
    "    from keras import callbacks\n",
    "    from keras import applications\n",
    "    from keras.models import model_from_json, Sequential\n",
    "    from keras.layers import Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, Activation\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is needed later when evaluating the classifier's results.\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=2, out=None)  \n",
    "    \n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03a.1 Parametrization\n",
    "\n",
    "To explore the possibilities of transfer learning, we need to define datapaths, image parameters as well as network parameters. This is done in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the path to our data. You'll only need the datasets when extracting features yourself. \n",
    "# You may also use the 'TRAIN_SIMPLE' / 'TEST_SIMPLE' dataset if your device can't handle larger datasets.\n",
    "\n",
    "train_data_dir = './datasets/data_blood/images/TRAIN/'\n",
    "test_data_dir = './datasets/data_blood/images/TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define image dimensions as tuple, the image data will be scaled \n",
    "# to the specified size, in this case we will use 160x120\n",
    "img_size = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epoches = 100\n",
    "patience = 10\n",
    "dropout_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the log folder for tensorboard (helps by visualizing training curves)\n",
    "logdir = \"transfer_learning_logs/\"\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callback in order to check for number of epochs defined in 'patience'\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-04, patience=patience, mode='auto')\n",
    "\n",
    "# define a tensorboard callback\n",
    "tb = callbacks.TensorBoard(log_dir=logdir + \"TransferLearning_\" + datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03a.2 Architecture Definition\n",
    "\n",
    "* Load the [VGG16](https://arxiv.org/abs/1409.1556) network's weights from Keras' `applications` module. You may also use other models, but you'd have to make sure to copy their architecture correctly on your own. Depending on the model, this may not be a trivial task.\n",
    "* Set the option `include_top=False` to load convolutional layers only and ignore the classification part of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define and load VGG16 model with imagenet-weights, exclude the fully connected layers and define the input shape\n",
    "base_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03a.3 Feature Extraction\n",
    "\n",
    "Use the base model to generate features using the images provided in the `datasets/data_blood` folder.\n",
    "You will use the extracted features as the input for your classification network.\n",
    "This technique is also called bottlenecking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag determines whether the feature extraction network is run or not.\n",
    "# You can either use the prepared features from moodle or run your own feature extractor.\n",
    "do_extract_features = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_extract_features == 1:\n",
    "\n",
    "    # TODO: define image data generator for training data\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # TODO: define image data generator for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # generate batches of train image data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=img_size,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # get filenames for training data\n",
    "    train_filenames = train_generator.filenames\n",
    "    # get number of training samples\n",
    "    train_samples = len(train_filenames)\n",
    "    # get batch size\n",
    "    predict_steps_train = int(np.ceil(train_samples / batch_size))\n",
    "\n",
    "    # generate batches of test image data\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        shuffle=False,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_filenames = test_generator.filenames\n",
    "    test_samples = len(test_filenames)\n",
    "    predict_steps_test = int(np.ceil(test_samples / batch_size))\n",
    "\n",
    "    print(\"Starting feature extraction for train images\")\n",
    "\n",
    "    # TODO: get number of classes\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    # TODO: get the class labels for the training data\n",
    "    train_labels = train_generator.classes\n",
    "    # TODO: convert the training labels to one-hot encoding aka. categorical vectors\n",
    "    train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
    "    # extract features using the training image generator\n",
    "    train_features= base_model.predict_generator(train_generator, predict_steps_train, verbose = 1)\n",
    "\n",
    "    # TODO: Saving the bottleneck features and their corresponding labels using e.g. numpy's savez()\n",
    "    np.savez('./datasets/data_blood/trainfeatures_full', features=train_features, labels=train_labels)\n",
    "    print(\"Starting feature extraction for test images\")\n",
    "    # TODO: extract and store the features for the test data\n",
    "\n",
    "    print(\"Feature extraction done. Bottleneck features saved.\")\n",
    "\n",
    "    # TODO: get the class labels for the training data\n",
    "    test_labels = test_generator.classes\n",
    "    # TODO: convert the training labels to one-hot encoding aka. categorical vectors\n",
    "    test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
    "    # extract features using the training image generator\n",
    "    test_features = base_model.predict_generator(test_generator, predict_steps_test, verbose = 1)\n",
    "\n",
    "    # TODO: Saving the bottleneck features and their corresponding labels using e.g. numpy's savez()\n",
    "    np.savez('./datasets/data_blood/testfeatures_full', features=test_features, labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03a.4 Architecture Definition and Classification\n",
    "\n",
    "Now it is time to create a classification network for the extracted features. Create a MLP with the necessary complexity and use `GlobalAveragePooling2D` as the first layer of your classification network. Think about the number of the nodes in the output layer.\n",
    "\n",
    "Try to get a stable accuracy of >92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the saved bottleneck training features\n",
    "npzfile = ...\n",
    "# TODO: get features\n",
    "train_features = ...\n",
    "# TODO: get labels\n",
    "train_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define classification part of the transfer model\n",
    "transfered_model = Sequential()\n",
    "\n",
    "...\n",
    "\n",
    "# compile the model with the Adam optimizer\n",
    "transfered_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# print model structure\n",
    "print(transfered_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: train the model using the bottleneck features generated in the feature extraction part\n",
    "transfered_model.fit(\n",
    "    ...,\n",
    "    epochs=max_epoches, shuffle = True, batch_size= 256, verbose = 1,\n",
    "    validation_split = 0.2,\n",
    "    callbacks= [early_stop, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the training curves you can now activate tensorboard in your docker container using the following command after navigating to the working directory (e.g. `/notebooks/<your-working-directory>/`): \n",
    "\n",
    "`tensorboard --logdir transfer_learning_logs --host 0.0.0.0`\n",
    "\n",
    "Please note that the `--logdir` parameter has to be the same as your `logdir` variable. \n",
    "\n",
    "Afterwards navigate to [http://localhost:6006](http://localhost:6006) in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LB03a.5 Evaluation\n",
    "Load the test features saved to the disk in  LB03a.3.\n",
    "Use the test data in order to evaluate the performance of your classification network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate the model on the test data bottleneck features\n",
    "#load the bottleneck training features\n",
    "npzfile_test = ...\n",
    "# TODO: load the bottleneck test features\n",
    "test_features = ...\n",
    "# TODO: get labels\n",
    "test_labels = ...\n",
    "\n",
    "predictions = ...\n",
    "# a hint here: test_labels are one-hot encoded (if not saved in another format)\n",
    "targets= ...\n",
    "\n",
    "# output accuracy score, classification report and confusion matrix\n",
    "print('Accuracy on test data: %.2f %%\\n' % (accuracy_score(targets, predictions)*100))\n",
    "\n",
    "# TODO: calculate the confusion matrix\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "# TODO: plot the confusion matrix\n",
    "plot_confusion_matrix(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
